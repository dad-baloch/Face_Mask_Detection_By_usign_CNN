{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection using Convolutional Neural Network (CNN)\n",
    "\n",
    "This project implements a  learning model to detect whether a person is wearing a face mask or not using image classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "**Dataset:** Face Mask Detection Dataset\n",
    "\n",
    "**Dataset Link:** https://www.kaggle.com/datasets/andrewmvd/face-mask-detection\n",
    "\n",
    "**Description:** This dataset contains images of people with and without face masks. The images are divided into two classes:\n",
    "- `with_mask`: Images of people wearing face masks\n",
    "- `without_mask`: Images of people not wearing face masks\n",
    "\n",
    "This binary classification task is crucial for automated face mask detection systems in public spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection & Overview\n",
    "\n",
    "**Objective:** Load the dataset, explore its structure, and understand the distribution of classes.\n",
    "\n",
    "We will:\n",
    "1. Import necessary libraries for data processing, visualization, and deep learning\n",
    "2. Load image file paths from both classes\n",
    "3. Count the number of images in each class\n",
    "4. Visualize the class distribution to check for imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "We import all necessary libraries at the beginning for better code organization:\n",
    "- **os**: For file and directory operations\n",
    "- **numpy**: For numerical computations and array operations\n",
    "- **matplotlib**: For data visualization\n",
    "- **cv2 (OpenCV)**: For image processing\n",
    "- **PIL**: For image loading and manipulation\n",
    "- **sklearn**: For train-test splitting\n",
    "- **tensorflow/keras**: For building the CNN model\n",
    "- **seaborn**: For advanced visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOJzrDWTeQdJ"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and processing\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset File Paths\n",
    "\n",
    "Loading the image filenames from both directories to understand the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBGGdi_RgPxw",
    "outputId": "c3ec1559-7b86-4c30-9fa3-8b5a2c51b7df"
   },
   "outputs": [],
   "source": [
    "with_mask_files = os.listdir('../DataSet/with_mask')\n",
    "print(with_mask_files[0:5])\n",
    "print(with_mask_files[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMwoGWvWgtTc",
    "outputId": "9360eb7b-b8be-4345-8ba8-726dc5f4f375"
   },
   "outputs": [],
   "source": [
    "without_mask_files = os.listdir('../DataSet/without_mask')\n",
    "print(without_mask_files[0:5])\n",
    "print(without_mask_files[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKoygojehFo_",
    "outputId": "c3502e91-d8c1-4171-8c9a-1be72170ae04"
   },
   "outputs": [],
   "source": [
    "mask_images_no = len(with_mask_files)\n",
    "print('number of mask images:', mask_images_no)\n",
    "without_mask_images = len(without_mask_files)\n",
    "print('number of without mask images:', without_mask_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Class Distribution\n",
    "\n",
    "Creating a bar chart to visualize the distribution of images across both classes. This helps identify any class imbalance that might affect model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart showing class distribution\n",
    "classes = ['With Mask', 'Without Mask']\n",
    "counts = [mask_images_no, without_mask_images]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(classes, counts, color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
    "plt.title('Class Distribution: Face Mask Detection Dataset', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "total_images = mask_images_no + without_mask_images\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"Total Images: {total_images}\")\n",
    "print(f\"With Mask: {mask_images_no} ({mask_images_no/total_images*100:.2f}%)\")\n",
    "print(f\"Without Mask: {without_mask_images} ({without_mask_images/total_images*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvoq2BE_iCXy"
   },
   "source": [
    "### Creating Labels for Classification\n",
    "\n",
    "Assigning numerical labels to each class for the classification task:\n",
    "- **With mask → 1** (Positive class)\n",
    "- **Without mask → 0** (Negative class)\n",
    "\n",
    "These labels will be used as target values during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L4Ut4VWhX84",
    "outputId": "b97457b3-4a88-4097-cf90-e80be35de665"
   },
   "outputs": [],
   "source": [
    "# create the labels\n",
    "\n",
    "with_mask_label = [1]*mask_images_no\n",
    "without_mask_label = [0]*without_mask_images\n",
    "\n",
    "print(with_mask_label[0:5])\n",
    "\n",
    "print(without_mask_label[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGd1Md_VuQ9u",
    "outputId": "fa74a0e1-5887-4421-f69f-cfed9365d9a8"
   },
   "outputs": [],
   "source": [
    "print('number of mask images labels:', len(with_mask_label))\n",
    "print(\"Witout mask lebels:\",len(without_mask_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HC5RZvoGubRh",
    "outputId": "1066b229-0ce9-46d8-ab98-edf3b0342230"
   },
   "outputs": [],
   "source": [
    "labels = with_mask_label + without_mask_label\n",
    "\n",
    "print(\"Total Labels: \",len(labels))\n",
    "print(labels[0:5])\n",
    "print(labels[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preprocessing\n",
    "\n",
    "**Objective:** Ensure data quality by checking for corrupted images and preparing images for model training.\n",
    "\n",
    "Key steps:\n",
    "1. Check for corrupted or unreadable images\n",
    "2. Resize all images to a uniform size (128x128 pixels)\n",
    "3. Convert images to RGB format for consistency\n",
    "4. Normalize pixel values to the range [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Corrupted Images\n",
    "\n",
    "Before processing, we check if all images can be opened and read properly. This prevents errors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for corrupted images\n",
    "corrupted_images = []\n",
    "\n",
    "print(\"Checking for corrupted images in 'with_mask' folder...\")\n",
    "with_mask_path = '../DataSet/with_mask/'\n",
    "for img_file in with_mask_files:\n",
    "    try:\n",
    "        img = Image.open(with_mask_path + img_file)\n",
    "        img.verify()  # Verify that it is an image\n",
    "    except Exception as e:\n",
    "        corrupted_images.append(('with_mask', img_file))\n",
    "        print(f\"Corrupted: {img_file} - {e}\")\n",
    "\n",
    "print(\"\\nChecking for corrupted images in 'without_mask' folder...\")\n",
    "without_mask_path = '../DataSet/without_mask/'\n",
    "for img_file in without_mask_files:\n",
    "    try:\n",
    "        img = Image.open(without_mask_path + img_file)\n",
    "        img.verify()  # Verify that it is an image\n",
    "    except Exception as e:\n",
    "        corrupted_images.append(('without_mask', img_file))\n",
    "        print(f\"Corrupted: {img_file} - {e}\")\n",
    "\n",
    "# Summary\n",
    "if len(corrupted_images) == 0:\n",
    "    print(\"\\n✓ All images are valid and readable!\")\n",
    "else:\n",
    "    print(f\"\\n✗ Found {len(corrupted_images)} corrupted image(s):\")\n",
    "    for folder, filename in corrupted_images:\n",
    "        print(f\"  - {folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8O57l8gu6uk"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Objective:** Understand the characteristics of our image data through statistical analysis and visualization.\n",
    "\n",
    "We will:\n",
    "1. Display sample images from both classes\n",
    "2. Analyze image dimensions and properties\n",
    "3. Visualize multiple samples to understand data variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Image Visualization\n",
    "\n",
    "Displaying sample images from both classes to understand the visual characteristics of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "AODhtH42uzui",
    "outputId": "e42284b9-69b9-4cb0-c9b4-0aa983379d5d"
   },
   "outputs": [],
   "source": [
    "# displaying with mask image\n",
    "\n",
    "img = mpimg.imread('../DataSet/with_mask/53.png')\n",
    "imgplot  = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "qsLEfW-nvwt0",
    "outputId": "a8b10663-9082-4e1a-d70e-8a037278e00e"
   },
   "outputs": [],
   "source": [
    "# displaying without mask image\n",
    "\n",
    "img = mpimg.imread('../DataSet/without_mask/99.png')\n",
    "imgplot  = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Multiple Samples\n",
    "\n",
    "Visualizing multiple samples from each class to better understand data variability and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display multiple samples from both classes\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Sample Images from Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Display 5 samples with mask\n",
    "for i in range(5):\n",
    "    img = mpimg.imread(f'../DataSet/with_mask/{with_mask_files[i]}')\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f'With Mask\\n{img.shape}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Display 5 samples without mask\n",
    "for i in range(5):\n",
    "    img = mpimg.imread(f'../DataSet/without_mask/{without_mask_files[i]}')\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].set_title(f'Without Mask\\n{img.shape}', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary of Images\n",
    "\n",
    "Analyzing the dimensions and properties of raw images before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image dimensions\n",
    "image_shapes = []\n",
    "\n",
    "# Sample 50 images from each class to analyze dimensions\n",
    "sample_size = min(50, len(with_mask_files))\n",
    "\n",
    "for i in range(sample_size):\n",
    "    img = mpimg.imread(f'../DataSet/with_mask/{with_mask_files[i]}')\n",
    "    image_shapes.append(img.shape)\n",
    "    \n",
    "for i in range(sample_size):\n",
    "    img = mpimg.imread(f'../DataSet/without_mask/{without_mask_files[i]}')\n",
    "    image_shapes.append(img.shape)\n",
    "\n",
    "# Convert to numpy array for analysis\n",
    "image_shapes = np.array(image_shapes)\n",
    "\n",
    "print(\"IMAGE DIMENSION STATISTICS\")\n",
    "print(f\"Sample size analyzed: {len(image_shapes)} images\")\n",
    "print(f\"\\nHeight statistics:\")\n",
    "print(f\"  Mean: {np.mean(image_shapes[:, 0]):.2f} pixels\")\n",
    "print(f\"  Min: {np.min(image_shapes[:, 0])} pixels\")\n",
    "print(f\"  Max: {np.max(image_shapes[:, 0])} pixels\")\n",
    "print(f\"\\nWidth statistics:\")\n",
    "print(f\"  Mean: {np.mean(image_shapes[:, 1]):.2f} pixels\")\n",
    "print(f\"  Min: {np.min(image_shapes[:, 1])} pixels\")\n",
    "print(f\"  Max: {np.max(image_shapes[:, 1])} pixels\")\n",
    "print(f\"\\nColor channels: {image_shapes[0, 2]} (RGB)\")\n",
    "print(\"\\n✓ This variability in dimensions confirms the need for resizing \\n all images to a uniform size (128x128) for model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG6OQo_Uw_ZN"
   },
   "source": [
    "##  Feature Engineering\n",
    "\n",
    "**Objective:** Transform raw images into a format suitable for CNN training.\n",
    "\n",
    "**Key transformations:**\n",
    "1. **Resizing**: Convert all images to 128x128 pixels for uniform input shape\n",
    "2. **RGB Conversion**: Ensure all images have 3 color channels (Red, Green, Blue)\n",
    "3. **Array Conversion**: Convert PIL images to NumPy arrays for numerical processing\n",
    "4. **Normalization**: Scale pixel values from [0, 255] to [0, 1] by dividing by 255\n",
    "\n",
    "**Why normalize?** Neural networks converge faster and perform better when input features are scaled to similar ranges. Normalization prevents features with larger values from dominating the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing Pipeline\n",
    "\n",
    "Loading, resizing, and converting images to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDnAKMktxBhI",
    "outputId": "38dae63f-0ff4-4854-8cd7-092e90c2fc47"
   },
   "outputs": [],
   "source": [
    "# convert images to numpy arrays\n",
    "\n",
    "with_mask_path = '../DataSet/with_mask/'\n",
    "\n",
    "data = []\n",
    "\n",
    "for img_file in with_mask_files:\n",
    "\n",
    "  image = Image.open(with_mask_path + img_file)\n",
    "  image = image.resize((128,128))\n",
    "  image = image.convert('RGB')\n",
    "  image = np.array(image)\n",
    "  data.append(image)\n",
    "\n",
    "\n",
    "\n",
    "without_mask_path = '../DataSet/without_mask/'\n",
    "\n",
    "\n",
    "for img_file in without_mask_files:\n",
    "\n",
    "  image = Image.open(without_mask_path + img_file)\n",
    "  image = image.resize((128,128))\n",
    "  image = image.convert('RGB')\n",
    "  image = np.array(image)\n",
    "  data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_J4uNoWwBj5",
    "outputId": "8775a9ab-e1fe-4d1c-d429-00897d40f91c"
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVG4ZNdlLhSO",
    "outputId": "017f6dd9-de14-4426-a6e9-1ec9e68da74c"
   },
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlqAmmo7LkNa",
    "outputId": "8890a810-04bf-4744-c755-67aadf4d3e21"
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZZz3B-EN2Qn",
    "outputId": "5f367971-75a8-4b4c-fa16-d78f1c52b7df"
   },
   "outputs": [],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDRrOS88OAGB",
    "outputId": "90dd7e79-2591-41c7-d34a-178628deb66e"
   },
   "outputs": [],
   "source": [
    "data[78].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyXMc8X4ODjL",
    "outputId": "a6ec7a4a-08e3-4642-87a5-96b9ca0053fa"
   },
   "outputs": [],
   "source": [
    "# converting images list and label list to numpy arrays\n",
    "\n",
    "X = np.array(data)\n",
    "Y = np.array(labels)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6jRK8piOm3g",
    "outputId": "a77bb093-c247-4e19-f7c8-365ecbb9ab21"
   },
   "outputs": [],
   "source": [
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybT3ETbtOo8s",
    "outputId": "5e2824f9-003c-42c6-9614-e554ac9e42ff"
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEGEeZPwOrMy",
    "outputId": "e168c077-3ffd-40bb-a92d-0677145d00ff"
   },
   "outputs": [],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0Y-7WjWOzlw",
    "outputId": "ab1a1d86-1e83-48ab-d691-7673c871530b"
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9v4LseHSREW"
   },
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Splitting the dataset into training (80%) and testing (20%) sets. The training set is used to train the model, while the testing set evaluates its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ao9-HyslPFh_",
    "outputId": "28ec92a8-130a-4717-8c86-32921cb1b887"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=.2, random_state=2)\n",
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wP7sEOYOSxq-"
   },
   "outputs": [],
   "source": [
    "# scaling the data\n",
    "\n",
    "X_train_scaled = X_train/255\n",
    "X_test_scaled = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Normalization (Scaling)\n",
    "\n",
    "**Why normalize?** We divide all pixel values by 255 to scale them from the range [0, 255] to [0, 1]. This normalization:\n",
    "- Helps the neural network converge faster during training\n",
    "- Prevents gradient explosion/vanishing problems\n",
    "- Makes the model more stable and improves generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXqA1rMrTcdi",
    "outputId": "d999d978-e268-4f53-df78-e47ded309cff"
   },
   "outputs": [],
   "source": [
    "X_train[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sRNKKzNTeya",
    "outputId": "77c54480-61b1-4dd1-c700-e8706845b6a8"
   },
   "outputs": [],
   "source": [
    "X_train_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N-ynvjKUY3G"
   },
   "source": [
    "## Model Building\n",
    "\n",
    "**Objective:** Design and train a Convolutional Neural Network (CNN) for binary image classification.\n",
    "\n",
    "**CNN Architecture:**\n",
    "- **Input Layer**: Accepts 128x128x3 images (height × width × RGB channels)\n",
    "- **Convolutional Layers**: Extract spatial features from images\n",
    "- **MaxPooling Layers**: Reduce spatial dimensions and computational cost\n",
    "- **Flatten Layer**: Convert 2D feature maps to 1D vector\n",
    "- **Dense Layers**: Learn complex patterns and make final predictions\n",
    "- **Dropout Layers**: Prevent overfitting by randomly dropping neurons during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN Architecture\n",
    "\n",
    "**Modern approach:** Using `keras.Input()` to define the input shape (recommended over `input_shape` in the first layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYEmCkYXVcMO"
   },
   "outputs": [],
   "source": [
    "# Define the CNN model using modern Keras Input layer\n",
    "num_of_classes = 2\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Input layer - explicitly define input shape\n",
    "    keras.Input(shape=(128, 128, 3)),\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the 2D features to 1D\n",
    "    keras.layers.Flatten(),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),  # Dropout for regularization\n",
    "    \n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Output layer\n",
    "    keras.layers.Dense(num_of_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSprtI-FYbgp"
   },
   "outputs": [],
   "source": [
    "# compile the Neural Network\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "\n",
    "**Optimizer:** Adam - An adaptive learning rate optimization algorithm\n",
    "**Loss Function:** Sparse Categorical Crossentropy - For integer-labeled classification\n",
    "**Metrics:** Accuracy - To track model performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3uQbCLKZL2y",
    "outputId": "aa6a007b-456d-4ba8-a471-fab0357080bc"
   },
   "outputs": [],
   "source": [
    "# training the neural network\n",
    "history = model.fit(X_train_scaled, Y_train, validation_split=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Trained Model\n",
    "\n",
    "Saving the model to disk allows us to:\n",
    "- Reuse the trained model without retraining\n",
    "- Deploy the model in production environments\n",
    "- Share the model with others\n",
    "\n",
    "The model is saved in HDF5 format (.h5 extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = 'face_mask_detection_model.h5'\n",
    "model.save(model_path)\n",
    "print(f\"✓ Model successfully saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Visualization - Training Performance\n",
    "\n",
    "**Objective:** Visualize the model's training progress to understand its learning behavior.\n",
    "\n",
    "We plot:\n",
    "1. **Loss curves**: Show how well the model fits the training data\n",
    "2. **Accuracy curves**: Show the model's prediction accuracy over epochs\n",
    "\n",
    "**What to look for:**\n",
    "- Decreasing loss indicates the model is learning\n",
    "- Gap between training and validation curves indicates overfitting\n",
    "- Converging curves indicate good generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training History Visualization\n",
    "\n",
    "Plotting loss and accuracy curves to understand the model's learning progress over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2, marker='o')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, marker='s')\n",
    "axes[0].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='upper right', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy\n",
    "axes[1].plot(history.history['acc'], label='Training Accuracy', linewidth=2, marker='o')\n",
    "axes[1].plot(history.history['val_acc'], label='Validation Accuracy', linewidth=2, marker='s')\n",
    "axes[1].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='lower right', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Training Accuracy: {history.history['acc'][-1]:.4f}\")\n",
    "print(f\"Validation Accuracy: {history.history['val_acc'][-1]:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Training the CNN on the dataset. We use:\n",
    "- **validation_split=0.1**: 10% of training data used for validation during training\n",
    "- **epochs=10**: The model will see the entire training dataset 10 times\n",
    "\n",
    "The `history` object stores training metrics which we'll use for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8oo_18Hc1n6"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "**Objective:** Assess the model's performance on unseen test data using multiple metrics.\n",
    "\n",
    "We will evaluate:\n",
    "1. **Test Accuracy**: Overall percentage of correct predictions\n",
    "2. **Confusion Matrix**: Detailed breakdown of correct and incorrect predictions\n",
    "3. **Classification Report**: Precision, Recall, and F1-Score for each class\n",
    "\n",
    "These metrics provide a comprehensive understanding of model performance beyond simple accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Accuracy\n",
    "\n",
    "Evaluating the model on the test set to measure its performance on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cx1FZ0UEakXh",
    "outputId": "dc54ff2c-93c7-4d9a-ab26-e38af93119f4"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test_scaled, Y_test)\n",
    "print('Test accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix shows:\n",
    "- **True Positives (TP)**: Correctly predicted \"with mask\"\n",
    "- **True Negatives (TN)**: Correctly predicted \"without mask\"\n",
    "- **False Positives (FP)**: Incorrectly predicted \"with mask\"\n",
    "- **False Negatives (FN)**: Incorrectly predicted \"without mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "Y_pred = model.predict(X_test_scaled)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Without Mask (0)', 'With Mask (1)'],\n",
    "            yticklabels=['Without Mask (0)', 'With Mask (1)'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Face Mask Detection', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix values\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"True Negatives (TN): {cm[0, 0]} - Correctly predicted 'Without Mask'\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]} - Incorrectly predicted 'With Mask'\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]} - Incorrectly predicted 'Without Mask'\")\n",
    "print(f\"True Positives (TP): {cm[1, 1]} - Correctly predicted 'With Mask'\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report: Precision, Recall, and F1-Score\n",
    "\n",
    "**Metrics explained:**\n",
    "- **Precision**: Of all predictions for a class, how many were correct? (TP / (TP + FP))\n",
    "- **Recall**: Of all actual instances of a class, how many did we find? (TP / (TP + FN))\n",
    "- **F1-Score**: Harmonic mean of Precision and Recall (balances both metrics)\n",
    "- **Support**: Number of actual occurrences of each class in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "class_names = ['Without Mask (0)', 'With Mask (1)']\n",
    "report = classification_report(Y_test, Y_pred_classes, target_names=class_names)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(report)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Interpretation\n",
    "\n",
    "**Summary of Results:**\n",
    "\n",
    "The classification report above provides detailed performance metrics for our Face Mask Detection model:\n",
    "\n",
    "1. **High Precision** means when the model predicts a class, it's usually correct\n",
    "2. **High Recall** means the model successfully identifies most instances of each class\n",
    "3. **High F1-Score** indicates a good balance between Precision and Recall\n",
    "4. **Overall Accuracy** shows the percentage of all predictions that were correct\n",
    "\n",
    "**Practical Implications:**\n",
    "- A high recall for \"With Mask\" is crucial - we want to catch most people wearing masks correctly\n",
    "- A high precision for \"Without Mask\" is important - we don't want false alarms\n",
    "- The confusion matrix helps identify which types of errors the model makes most often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EY8QRPKe7t1"
   },
   "source": [
    "## Predictive System (Inference)\n",
    "\n",
    "**Objective:** Use the trained model to make predictions on new, unseen images.\n",
    "\n",
    "**Process:**\n",
    "1. Load a new image\n",
    "2. Resize it to 128x128 pixels\n",
    "3. Normalize pixel values (divide by 255)\n",
    "4. Reshape to match model input format\n",
    "5. Make prediction and interpret results\n",
    "\n",
    "**Note:** For Jupyter notebooks, use `plt.imshow()` instead of `cv2_imshow()` to display images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Import Libraries First\n",
    "\n",
    "**Important:** Before running the prediction cell below, make sure you have executed cell 5 to import all required libraries (cv2, numpy, matplotlib, etc.). \n",
    "\n",
    "If you get a \"cv2 is not defined\" error, go back and run cell 5 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "4z81QKFhhZZ2",
    "outputId": "10b18dda-c04d-4a30-950e-fe7a99255767"
   },
   "outputs": [],
   "source": [
    "input_image_path = input('Path of the image to be predicted: ')\n",
    "\n",
    "input_image = cv2.imread(input_image_path)\n",
    "\n",
    "# Convert BGR to RGB for proper display and prediction\n",
    "input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(input_image_rgb)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Preprocess the image\n",
    "input_image_resized = cv2.resize(input_image_rgb, (128, 128))\n",
    "input_image_scaled = input_image_resized / 255\n",
    "input_image_reshaped = np.reshape(input_image_scaled, [1, 128, 128, 3])\n",
    "\n",
    "# Make prediction\n",
    "input_prediction = model.predict(input_image_reshaped)\n",
    "print(f\"\\nPrediction probabilities: {input_prediction}\")\n",
    "\n",
    "input_pred_label = np.argmax(input_prediction)\n",
    "print(f\"Predicted class: {input_pred_label}\")\n",
    "\n",
    "# Display result\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if input_pred_label == 1:\n",
    "    print('✓ The person in the image is wearing a mask.')\n",
    "else:\n",
    "    print('✗ The person in the image is not wearing a mask.')\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Using the Saved Model\n",
    "\n",
    "**Why load a saved model?**\n",
    "- Reuse the trained model without retraining (saves time and computational resources)\n",
    "- Deploy the model in production\n",
    "- Share the model with others\n",
    "- Continue predictions in a new session\n",
    "\n",
    "**Important:** When loading a saved model, you only need to import the necessary libraries. You don't need to retrain or have access to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Load Model and Predict on a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "loaded_model = keras.models.load_model('face_mask_detection_model.h5')\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "name = input(\"Enter the name of the image (.png): \")\n",
    "image_path = f'../test/{name}.png' \n",
    "input_image = cv2.imread(image_path)\n",
    "input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(input_image_rgb)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "input_image_resized = cv2.resize(input_image_rgb, (128, 128))\n",
    "input_image_scaled = input_image_resized / 255.0\n",
    "input_image_reshaped = np.reshape(input_image_scaled, [1, 128, 128, 3])\n",
    "\n",
    "prediction = loaded_model.predict(input_image_reshaped)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "print(f\"\\nPrediction probabilities: {prediction}\")\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if predicted_class == 1:\n",
    "    print('✓ The person in the image is wearing a mask.')\n",
    "else:\n",
    "    print('✗ The person in the image is not wearing a mask.')\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
